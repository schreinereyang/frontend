// pages/api/chat.js

import OpenAI from "openai";
import funnel from "../../utils/funnel.json";
import { extractMemoryFromMessage } from "../../utils/memory";
import { getCurrentPhase, getRandomMessage } from "../../utils/phaseEngine";
import { liaPersona } from "../../utils/liaPersona";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export default async function handler(req, res) {
  const { message } = req.body;

  // Initialiser la m√©moire globale si elle n'existe pas
  if (!global.memory) {
    global.memory = {
      name: null,
      age: null,
      isAlone: null,
      lastMessages: [],
      purchaseMade: false,
    };
  }

  // Mise √† jour de la m√©moire
  global.memory = extractMemoryFromMessage(message, global.memory);
  console.log("M√©moire actuelle :", global.memory);

  // D√©tection de la phase
  const currentPhase = getCurrentPhase(global.memory, funnel, message);
  console.log("Phase actuelle d√©tect√©e :", currentPhase?.name);

  // G√©n√©ration de r√©ponse simple (sans GPT si phase contient un message √©crit)
  const aiReply = getRandomMessage(currentPhase, "fr"); // TODO : d√©tecter "en" ou "fr" dynamiquement

  // (optionnel) Utiliser GPT uniquement si aucun message n‚Äôest d√©fini
  if (aiReply === "...") {
    const memoryContext = `
Fan:
- Pr√©nom: ${global.memory.name || "inconnu"}
- √Çge: ${global.memory.age || "inconnu"}
- Est seul ? ${global.memory.isAlone === true ? "oui" : global.memory.isAlone === false ? "non" : "inconnu"}
`;

    const promptSystem = `
Tu es ${liaPersona.name}, une mod√®le OnlyFans ${liaPersona.tone} de ${liaPersona.origin}, 20 ans.
Objectif : ${liaPersona.goal}

Voici ce que tu sais sur le fan :
${memoryContext}

R√®gles :
- Ne repose pas une question d√©j√† pos√©e si tu connais la r√©ponse
- Adapte-toi √† ses mots
- S‚Äôil n‚Äôest pas seul, passe en mode "copine virtuelle" sans vendre

Style : ${liaPersona.style}
Personnalit√© : ${liaPersona.personality}
`;

    try {
      const completion = await openai.chat.completions.create({
        model: "gpt-4",
        messages: [
          { role: "system", content: promptSystem },
          { role: "user", content: message },
        ],
      });

      const gptReply = completion.choices?.[0]?.message?.content || "Je ne suis pas s√ªre d‚Äôavoir bien compris üòò";
      return res.status(200).json({ reply: gptReply });
    } catch (error) {
      console.error("Erreur GPT:", error);
      return res.status(500).json({ error: "Erreur GPT" });
    }
  }

  // Sinon, r√©ponse g√©n√©r√©e depuis le funnel
  res.status(200).json({ reply: aiReply });
}
